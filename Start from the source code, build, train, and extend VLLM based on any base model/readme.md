I have currently combined the Qwen2VL visual encoder (650M) with the Llama3.2 (1B) language model, resulting in a Vision-Language Language Model (VLLM). This work encompasses:
### Coming Soon
- Pre-training
- Downstream fine-tuning (including full fine-tuning and LoRA fine-tuning, among other methods)
- Evaluation
- Web UI development
- Chain-of-Thought (COT)
- Retrieval-Augmented Generation (RAG)
- Quantization
- Formatted to meet the requirements of top-tier conference papers.

After completing all the work, I will refactor the code and add detailed comments.
