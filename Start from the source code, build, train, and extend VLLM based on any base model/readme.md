I have currently combined the Qwen2VL visual encoder (650M) with the Llama3.2 (1B) language model, resulting in a VLLM.
### Coming Soon
- Pre-training -- done
- SFT -- done
- RLHF -- done
- Downstream fine-tuning (including full fine-tuning and LoRA fine-tuning, among other methods)
- Evaluation(benchmark) -- done
- Web UI development -- done
- Chain-of-Thought (COT)
- Retrieval-Augmented Generation (RAG) -- done
- Quantization

After completing all the work, I will refactor the code and add detailed comments.
