I have currently combined the Qwen2VL visual encoder (650M) with the Llama3.2 (1B) language model, resulting in a VLLM.
### Coming Soon
- Pre-training
- RLHF
- Downstream fine-tuning (including full fine-tuning and LoRA fine-tuning, among other methods)
- Evaluation
- Web UI development
- Chain-of-Thought (COT)
- Retrieval-Augmented Generation (RAG)
- Quantization

After completing all the work, I will refactor the code and add detailed comments.
