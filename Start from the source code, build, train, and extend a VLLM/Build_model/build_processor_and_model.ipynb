{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processor of the new model consists of two parts: the first part uses Qwen2VL's image processing module, while the second part adopts LLaMA's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "model_id = \"/home/zhuyao/Sunpeng/models/llama3.2_1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "tokenizer.eos_token = tokenizer.eos_token if tokenizer.eos_token else \"<|endoftext|>\"\n",
    "\n",
    "special_tokens_dict = {'pad_token': '<|pad|>',\n",
    "                       'additional_special_tokens':[\"<image>\",\"<image_start>\",\"<image_end>\"]}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "tokenizer.save_pretrained(\"/home/zhuyao/Sunpeng/llava_qwen/storage_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. By this point the new part of the model's processor has been processed, and the next step is to look at the calls in model_processing.py.\n",
    "2. Make sure to change the model's default system_message to \"You are a helpful assistant.\" Modifying it directly in the saved tokenizer's config file is probably the most convenient approach.\n",
    "3. It should be noted that I did not replace the tokenizer of Qwen2VL's processor but instead combined its image processing module with LLaMA's tokenizer. In fact, replacing the tokenizer of Qwen2VL's processor should also be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the two base models of the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "model_qwen = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"/home/zhuyao/Sunpeng/models/qwen_2B_instruct\", torch_dtype=\"auto\", device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "new_linear = nn.Linear(in_features=5120, out_features=2048, bias=True) \n",
    "model_qwen.visual.merger.mlp[2] = new_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "model_id = \"/home/zhuyao/Sunpeng/models/llama3.2_1B\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/zhuyao/Sunpeng/llava_qwen/storage_model\", use_fast=True)\n",
    "\n",
    "tokenizer.eos_token = tokenizer.eos_token if tokenizer.eos_token else \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token else tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "model_llama.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and save the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made some modifications to the LLaMA source code. Specifically, I created a new config class, added a vision model to the model component of LLaMA, and introduced additional steps for processing inputs (handling image embeddings). If you're curious about this part, you can compare my code with LLaMA's original files. The changes I made are not particularly extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/zhuyao/Sunpeng/llava_qwen/SP\")\n",
    "from model.configuration_qwen_llama import  LlamaConfig\n",
    "import json\n",
    "\n",
    "with open(\"./init_config.json\", \"r\") as f:\n",
    "    model_config_file = json.load(f)\n",
    "model_config = LlamaConfig(**model_config_file)\n",
    "\n",
    "from model.modeling_qwen_llama import LlamaForCausalLM\n",
    "model = LlamaForCausalLM(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = model_llama\n",
    "model.visual = model_qwen.visual\n",
    "model.to(device=\"cpu\")\n",
    "model.save_pretrained(\"/home/zhuyao/Sunpeng/llava_qwen/tes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import safe_open, save_file\n",
    "\n",
    "\n",
    "input_file = \"/home/zhuyao/Sunpeng/llava_qwen/tes/model.safetensors\"\n",
    "output_file = \"/home/zhuyao/Sunpeng/llava_qwen/tes/model.safetensors\"\n",
    "data = {}\n",
    "metadata = None\n",
    "with safe_open(input_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "    metadata = f.metadata() \n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "        modified_key = key.replace('model.model.', 'model.').replace('visual.model.', 'visual.').replace('visual.model.', 'visual.').replace('visual.visual.', 'visual.')\n",
    "        print(modified_key)\n",
    "        data[modified_key] = f.get_tensor(key)\n",
    "    data['lm_head.weight'] = data['model.embed_tokens.weight'].clone() # No tie_weights()!\n",
    "save_file(data, output_file, metadata=metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmarkllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
