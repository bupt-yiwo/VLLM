{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据并行，流水并行，张量并行,多维混合并行等等\n",
    "\n",
    "https://juejin.cn/post/7269698032655728640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cnblogs.com/CircleWang/p/15620825.html\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/86441879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5,6,7' # 指定该程序可以识别的物理GPU编号\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyModel()\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without tainer\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '7860'\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def main(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "\n",
    "    print(f\"Process {rank} initialized successfully!\")\n",
    "    model = torch.nn.Linear(10, 1).to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    inputs = torch.randn(20, 10).to(rank)\n",
    "    targets = torch.randn(20, 1).to(rank)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ddp_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup() # PyTorch 分布式训练中用于清理和终止分布式进程组的函数\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4  \n",
    "    torch.multiprocessing.spawn(main, args=(world_size,), nprocs=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tainer\n",
    "\n",
    "...\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "...\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, collate_fn=collate_func, sampler=DistributedSampler(trainset))\n",
    "validloader = DataLoader(validset, batch_size=64, collate_fn=collate_func, sampler=DistributedSampler(validset))\n",
    "...\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "model = xxx\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(int(os.environ[\"LOCAL_RANK\"]))\n",
    "model = DDP(model)\n",
    "...\n",
    "def print_rank_0(info):\n",
    "    if int(os.environ[\"RANK\"]) == 0:\n",
    "        print(info)\n",
    "...\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    acc_num = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(int(os.environ[\"LOCAL_RANK\"])) for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            acc_num += (pred.long() == batch[\"labels\"].long()).float().sum()\n",
    "    dist.all_reduce(acc_num)\n",
    "    return acc_num / len(validset)\n",
    "\n",
    "def train(epoch=3, log_step=100):\n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        trainloader.sampler.set_epoch(ep)\n",
    "        for batch in trainloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(int(os.environ[\"LOCAL_RANK\"])) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0:\n",
    "                dist.all_reduce(loss, op=dist.ReduceOp.AVG)\n",
    "                print_rank_0(f\"ep: {ep}, global_step: {global_step}, loss: {loss.item()}\")\n",
    "            global_step += 1\n",
    "        acc = evaluate()\n",
    "        print_rank_0(f\"ep: {ep}, acc: {acc}\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accelerate: https://huggingface.co/docs/accelerate/main/en/usage_guides/explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初级使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from accelerate import Accelerator ## accelerate\n",
    "accelerator = Accelerator() ## accelerate\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = accelerator.prepare(## accelerate\n",
    "    model, optimizer, training_dataloader, scheduler\n",
    ")\n",
    "\n",
    "for batch in training_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    accelerator.backward(loss)## accelerate\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认路径: ~/.cache/huggingface/accelerate/\\n\n",
    "! accelerate config\n",
    "! accelerate launch {xx.py}\n",
    "! accelerate launch --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision = \"bf16\") # choice1\n",
    "! accelerator config && choice bf16 # choice2\n",
    "! accelerator = launch --mixed_precision bf16 {xx.py} # choice2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度累计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=xx)\n",
    "...\n",
    "with accelerator.accumulate(model):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日志记录（Tensorboard）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(log_with=\"tensorboard\", project_dir=\"xx\")\n",
    "...\n",
    "accelerator.init_trackers(project_name=\"xx\")\n",
    "...\n",
    "accelerator.end_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.unwrap_model(model).save_pretrained(\n",
    "    save_directory=accelerator.project_dir + f\"/step_{global_step}/model\",\n",
    "    is_main_process=accelerator.is_main_process,\n",
    "    state_dict=accelerator.get_state_dict(model),\n",
    "    save_func=accelerator.save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 断点续训"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.save_state()\n",
    "accelerator.load_state()\n",
    "# 计算resume_epoch和resume_step\n",
    "# accelerator.skip_first_batches(trainloader,resume_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index][\"review\"], self.data.iloc[index][\"label\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def prepare_dataloader():\n",
    "\n",
    "    dataset = MyDataset()\n",
    "\n",
    "    trainset, validset = random_split(dataset, lengths=[0.9, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"/gemini/code/model\")\n",
    "\n",
    "    def collate_func(batch):\n",
    "        texts, labels = [], []\n",
    "        for item in batch:\n",
    "            texts.append(item[0])\n",
    "            labels.append(item[1])\n",
    "        inputs = tokenizer(texts, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        inputs[\"labels\"] = torch.tensor(labels)\n",
    "        return inputs\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=32, collate_fn=collate_func, shuffle=True)\n",
    "    validloader = DataLoader(validset, batch_size=64, collate_fn=collate_func, shuffle=False)\n",
    "\n",
    "    return trainloader, validloader\n",
    "\n",
    "\n",
    "def prepare_model_and_optimizer():\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"/gemini/code/model\")\n",
    "\n",
    "    lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def evaluate(model, validloader, accelerator: Accelerator):\n",
    "    model.eval()\n",
    "    acc_num = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            output = model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            pred, refs = accelerator.gather_for_metrics((pred, batch[\"labels\"]))\n",
    "            acc_num += (pred.long() == refs.long()).float().sum()\n",
    "    return acc_num / len(validloader.dataset)\n",
    "\n",
    "\n",
    "def train(model, optimizer, trainloader, validloader, accelerator: Accelerator, resume, epoch=3, log_step=10):\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    resume_step = 0\n",
    "    resume_epoch = 0\n",
    "\n",
    "    if resume is not None: # 断点续训\n",
    "        accelerator.load_state(resume)\n",
    "        steps_per_epoch = math.ceil(len(trainloader) / accelerator.gradient_accumulation_steps)\n",
    "        resume_step = global_step = int(resume.split(\"step_\")[-1])\n",
    "        resume_epoch = resume_step // steps_per_epoch\n",
    "        resume_step -= resume_epoch * steps_per_epoch\n",
    "        accelerator.print(f\"resume from checkpoint -> {resume}\")\n",
    "\n",
    "    for ep in range(resume_epoch, epoch):\n",
    "        model.train()\n",
    "        if resume and ep == resume_epoch and resume_step != 0: # 断点续训\n",
    "            active_dataloader = accelerator.skip_first_batches(trainloader, resume_step * accelerator.gradient_accumulation_steps)\n",
    "        else:\n",
    "            active_dataloader = trainloader\n",
    "        for batch in active_dataloader:\n",
    "            with accelerator.accumulate(model): # 梯度累计\n",
    "                optimizer.zero_grad()\n",
    "                output = model(**batch)\n",
    "                loss = output.loss\n",
    "                accelerator.backward(loss) # accelerator的backward\n",
    "                optimizer.step()\n",
    "\n",
    "                if accelerator.sync_gradients:# 梯度累计时更新step\n",
    "                    global_step += 1\n",
    "\n",
    "                    if global_step % log_step == 0:\n",
    "                        loss = accelerator.reduce(loss, \"mean\")\n",
    "                        accelerator.print(f\"ep: {ep}, global_step: {global_step}, loss: {loss.item()}\")\n",
    "                        accelerator.log({\"loss\": loss.item()}, global_step) # 日志\n",
    "\n",
    "                    if global_step % 50 == 0 and global_step != 0:\n",
    "                        accelerator.print(f\"save checkpoint -> step_{global_step}\")\n",
    "                        accelerator.save_state(accelerator.project_dir + f\"/step_{global_step}\") # 断点保存\n",
    "                        accelerator.unwrap_model(model).save_pretrained( # 模型保存\n",
    "                            save_directory=accelerator.project_dir + f\"/step_{global_step}/model\",\n",
    "                            is_main_process=accelerator.is_main_process,\n",
    "                            state_dict=accelerator.get_state_dict(model),\n",
    "                            save_func=accelerator.save\n",
    "                        )\n",
    "        acc = evaluate(model, validloader, accelerator)\n",
    "        accelerator.print(f\"ep: {ep}, acc: {acc}, time: {time.time() - start_time}\")\n",
    "        accelerator.log({\"acc\": acc}, global_step) # 日志\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=2, log_with=\"tensorboard\", project_dir=\"ckpts\") #初始化accelerator\n",
    "\n",
    "    accelerator.init_trackers(\"runs\") #初始化日志\n",
    "\n",
    "    trainloader, validloader = prepare_dataloader()\n",
    "\n",
    "    model, optimizer = prepare_model_and_optimizer()\n",
    "\n",
    "    model, optimizer, trainloader, validloader = accelerator.prepare(model, optimizer, trainloader, validloader) # accelerator多卡处理\n",
    "\n",
    "    train(model, optimizer, trainloader, validloader, accelerator, resume=\"/gemini/code/ckpts/step_150\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accelerate集成deepspeed（日常使用推荐）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! accelerate config 指定一下是否使用deepspeed以及对应json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepspeed: https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
