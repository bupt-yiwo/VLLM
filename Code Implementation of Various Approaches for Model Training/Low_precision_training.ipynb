{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "浮点数格式：FP64, FP32, FP16, BFLOAT16, TF32之间的相互区别\n",
    "\n",
    "https://www.cnblogs.com/lemonzhang/p/17843336.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 低精度训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(..., torch_dtype=torch.half) # float16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    ...,\n",
    "    adam_epsilon = 1e-4,# 完全fp16时梯度爆炸可以设置这个\n",
    ")\n",
    "\n",
    "# 如果用了lora，新的模块会是FP32，一般没必要改到16\n",
    "model = model.half()  # 当整个模型都是半精度时，需要将adam_epsilon调大\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"/home/zhuyao/Sunpeng/models/qwen_2B_instruct\", torch_dtype=torch.int8, device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# lora后可转换\n",
    "# model = model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name,param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_41956187/article/details/136997432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8bit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cnblogs.com/LXP-Never/p/16822727.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 启用 8-bit 量化 [−128,127]\n",
    "    llm_int8_threshold=6.0,  # 设置低精度计算的阈值：当某个权重的绝对值超过这个阈值时，认为它对模型性能的影响较大，将其保留为高精度类型。\n",
    "    llm_int8_skip_modules=None  # 指定跳过量化的模块（可选）\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"/home/zhuyao/Sunpeng/models/qwen_2B_instruct\", torch_dtype=torch.bfloat16, device_map=\"cuda\", quantization_config=quantization_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/zh/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 启用 4-bit 量化 [-8, 7]\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # 计算时的精度:量化主要针对模型的权重存储，而实际计算仍然需要浮点数来保持精度。\n",
    "    bnb_4bit_use_double_quant=True,  # 双重量化:4-bit 权重的值被先量化为 int8，然后再进一步压缩到 4-bit。\n",
    "    bnb_4bit_quant_type=\"nf4\"  # 量化类型（如 `nf4` 或标准量化） NF4 使用对数分布或其他非均匀分布来更精确地表示权重值。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QLora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在量化的模型上插入 LoRA 层，微调这些低秩参数。量化后的权重在反向传播时被临时去量化（恢复为高精度），以确保梯度计算的精度。\n",
    "显存占用极低，支持超大模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_name = \"huggingface/llama-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
