{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高效微调\n",
    "总结：\n",
    "1. XXConfig(task_type=XXX)\n",
    "2. get_peft_model(model, config)\n",
    "3. PeftModel.from pretrained(model, model id)\n",
    "4. peft model.merge and unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.BitFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BitFit（Bias-term Fine-tuning）是一种参数高效的微调方法，专注于仅调整预训练模型中的偏置项（bias），而冻结其他参数。这种方法在中小规模数据集上表现出色，能够在仅更新约0.08%的参数情况下，达到与全量微调相当的效果。 \n",
    "\n",
    "主要特点：\n",
    "\n",
    "参数更新量小： 仅调整模型的偏置项，显著减少了需要更新的参数数量。\n",
    "\n",
    "适用性广： 在中小规模数据集上，BitFit的性能与全量微调相当，甚至在某些任务上表现更佳。\n",
    "\n",
    "资源友好： 由于更新的参数量极少，适合在内存受限的环境中部署，为硬件部署提供了可能性。\n",
    "\n",
    "应用场景： BitFit适用于需要在有限资源下进行模型微调的场景，特别是在中小规模数据集上，能够在保持性能的同时，降低计算和存储成本。\n",
    "\n",
    "限制场景： 对偏置项敏感的任务（如分类任务）表现较好，但对于需要调整大量模型参数的复杂任务（如生成任务、问答系统）可能效果有限。\n",
    "偏置项对模型功能的贡献有限，不能充分捕获任务相关的复杂特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结所有参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 只解冻偏置项\n",
    "for name, param in model.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以理解为给模型的 embed_tokens 层扩展出 N 个（或者指定数量的）虚拟 token 对应的嵌入向量，然后将这些虚拟 token 的嵌入拼接到输入序列前，作为模型的完整输入。\n",
    "\n",
    "优点\n",
    "\n",
    "参数高效：只优化少量 Prompt 嵌入 (<1%)，节省显存和计算资源。\n",
    "\n",
    "适配灵活：冻结模型主干，适配多个任务无需修改原始模型。\n",
    "\n",
    "训练效率高：只训练 Prompt 嵌入，训练速度快，开销小。\n",
    "\n",
    "适合生成任务：在文本生成任务中（如翻译、摘要）效果尤为显著。\n",
    "\n",
    "缺点\n",
    "\n",
    "表达能力有限：对复杂任务（如分布迁移）效果不如 LoRA 或全量微调。\n",
    "\n",
    "依赖预训练模型：性能高度依赖模型的原始表示能力。\n",
    "\n",
    "Prompt 长度需调优：长度过短效果不佳，过长可能引入噪声。\n",
    "\n",
    "泛化能力受限：少量参数可能导致过拟合。\n",
    "\n",
    "应用场景\n",
    "\n",
    "低资源适配：显存或计算受限时对大模型进行任务微调。\n",
    "\n",
    "多任务支持：为每个任务训练独立 Prompt 嵌入。\n",
    "\n",
    "Few-Shot/Zero-Shot 学习：少量或无标注数据的任务。\n",
    "\n",
    "领域适配：特定领域（如医疗、法律）的迁移学习。\n",
    "\n",
    "生成任务：翻译、摘要、格式化输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 实现\n",
    "from peft import PromptTuningConfig, get_peft_model, TaskType\n",
    "# 配置 Prompt Tuning\n",
    "config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # SEQ_CLS SEQ_2_SEQ_LM  CAUSAL_LM\n",
    "    prompt_length=10, # 一般从 10-50 的范围开始实验，根据验证集效果调整。\n",
    "    num_virtual_tokens=10,  \n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非PEFT实现\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 冻结所有参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 添加可学习的 Prompt 嵌入\n",
    "prompt_length = 5  \n",
    "hidden_size = model.config.hidden_size\n",
    "prompt_embeddings = torch.nn.Embedding(prompt_length, hidden_size)\n",
    "torch.nn.init.normal_(prompt_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "# 定义训练输入\n",
    "inputs = tokenizer(\"The movie was great!\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# 拼接可学习 Prompt\n",
    "prompt_ids = torch.arange(prompt_length).unsqueeze(0)\n",
    "prompt_vectors = prompt_embeddings(prompt_ids)\n",
    "prompt_vectors = prompt_vectors.repeat(input_ids.size(0), 1, 1)\n",
    "inputs_embeds = model.base_model.embeddings(input_ids)\n",
    "inputs_embeds = torch.cat([prompt_vectors, inputs_embeds], dim=1)\n",
    "\n",
    "\n",
    "outputs = model(inputs_embeds=inputs_embeds, labels=torch.tensor([1]))\n",
    "loss = outputs.loss\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的是soft prompt，还有hard prompt，即以真实的文本形式出现，如 \"Classify the sentiment of this sentence: [INPUT]\"，这种方式不训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Tuning v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Tuning v2 是一种改进的 Prompt Tuning 技术，主要解决了原始 Prompt Tuning 方法在小规模模型和复杂任务上的局限性，同时在大模型上进一步提升了性能。\n",
    "\n",
    "P-Tuning v2 的核心改进\n",
    "\n",
    "支持任意规模模型：\n",
    "\n",
    "原始 Prompt Tuning 的性能依赖于大模型（如 GPT-3），而 P-Tuning v2 通过优化结构和训练流程，使得小模型（如 BERT）也能从 Prompt Tuning 中受益。\n",
    "优化的提示嵌入插入位置：\n",
    "\n",
    "不局限于将 Prompt 嵌入放在输入序列的开头，而是可以动态插入到模型的多个层中（如 Transformer 的中间层）。\n",
    "这种分层插入（Layer-Wise Prompt Injection）机制提升了 Prompt 的表达能力。\n",
    "结合深层 Transformer 表示：\n",
    "\n",
    "Prompt 不再仅通过输入引导模型，而是直接参与深层 Transformer 的内部信息表示。\n",
    "通过与不同层的信息交互，Prompt 嵌入能够捕获更复杂的任务相关特征。\n",
    "优化训练策略：\n",
    "\n",
    "通过更高效的初始化和优化策略，使 Prompt 嵌入更快收敛，同时提高泛化性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT实现\n",
    "p_tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  \n",
    "    prompt_length=20,            \n",
    "    num_virtual_tokens=20,       \n",
    "    encoder_hidden_states=False, \n",
    "    insert_into_layers=\"deep\",   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  \n",
    "    prompt_length=30,                 \n",
    "    num_virtual_tokens=30,            \n",
    "    encoder_hidden_states=False,       \n",
    "    insert_into_layers=\"custom\"       \n",
    ")\n",
    "\n",
    "\n",
    "p_tuning_config.layers_to_insert = [0, 3, 7, 10]  \n",
    "p_tuning_config.prompt_location = \"start\"    # strart middle end，middle需要自定义中间插入逻辑\n",
    "p_tuning_config.shared_prompt = True    # False适合复杂任务         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "浅插入（shallow）：提示嵌入只在输入层使用，后续层不再加入提示嵌入。\n",
    "\n",
    "深插入（deep）：提示嵌入被动态插入到多个 Transformer 中间层，增强提示信息的影响力。\n",
    "\n",
    "encoder_hidden_states：如果任务需要处理多模态数据（如图像和文本），图像特征或文本特征可以作为额外隐藏状态，增强提示嵌入的表达能力。\n",
    "\n",
    "示例：\n",
    "图像描述生成任务中，图像的编码器隐藏状态可以传递给提示嵌入，指导文本生成。\n",
    "\n",
    "如果任务简单且资源受限，推荐设置为 False；如果任务复杂且需要丰富的上下文信息，建议启用 True\n",
    "\n",
    "自回归模型不分编码器和解码器，encoder_hidden_states 无意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和P-Tuning对比，参考https://blog.csdn.net/weixin_43863869/article/details/134760405\n",
    "\n",
    "Prefix-Tuning 的训练部分是前缀嵌入（Prefix Embedding），即加入到 Transformer 每一层自注意力机制中的一组可学习参数。\n",
    "这部分参数是独立于模型主体的，只有前缀嵌入会被优化。\n",
    "\n",
    "Prefix-Tuning 更适合生成任务和复杂任务，而 P-Tuning 更适合分类任务和简单场景。根据任务需求选择合适的方法即可！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT实现\n",
    "from peft import PrefixTuningConfig, get_peft_model, TaskType\n",
    "\n",
    "prefix_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  \n",
    "    num_virtual_tokens=20,            \n",
    "    encoder_hidden_states=False        \n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, prefix_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT实现，非PEFT实现不推荐写，会有可训练部分的问题要处理，这个轮子比较好用\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True  \n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=8,\n",
    "        target_modules=[\"q_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    for name, param in model.model.named_parameters():\n",
    "        if 'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "# 加载\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_save_path)\n",
    "#合并保存\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分流，禁用\n",
    "import torch.nn as nn\n",
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "config1 = LoraConfig(target_modules=[\"0\"])\n",
    "model2 = get_peft_model(net2, config1)\n",
    "model2.save_pretrained(\"./loraA\")\n",
    "\n",
    "config2 = LoraConfig(target_modules=[\"2\"])\n",
    "model2 = get_peft_model(net2, config2)\n",
    "model2.save_pretrained(\"./loraB\")\n",
    "\n",
    "model2 = PeftModel.from_pretrained(net2, model_id=\"./loraA/\", adapter_name=\"taskA\")\n",
    "model2.load_adapter(\"./loraB/\", adapter_name=\"taskB\")\n",
    "\n",
    "model2.active_adapter # 此时是taskA\n",
    "model2.set_adapter(\"taskB\")\n",
    "with model2.disable_adapter():\n",
    "    print(model2(torch.arange(0, 10).view(1, 10).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考https://blog.csdn.net/LLMUZI123456789/article/details/136880839\n",
    "\n",
    "IA3 (Infused Adapter by Attention and Activation) 是一种高效的参数微调方法，专为大规模预训练模型（如 Transformers）设计。IA3 的核心思想是：通过引入 可学习的标量缩放因子，调整每一层中 注意力权重（Attention） 和 前馈激活值（Activation） 的幅度，而不修改模型本身的参数。\n",
    "\n",
    "缩放 Attention 和 FFN，适合注意力主导的任务（如分类、生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT实现\n",
    "from peft import IA3Config, get_peft_model, TaskType, PeftModel\n",
    "ia3_config = IA3Config(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  \n",
    "    target_modules=[\"v_proj\", \"ffn\"], \n",
    "    ia3_alpha=8,                      \n",
    "    bias=\"none\",                      \n",
    ")\n",
    "\n",
    "ia3_model = get_peft_model(model, ia3_config)\n",
    "ia3_model.print_trainable_parameters()\n",
    "ia3_model = PeftModel.from_pretrained(base_model, \"./ia3_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认设置：ia3_alpha=8(÷8)\n",
    "\n",
    "对任务敏感时：减小 ia3_alpha，例如设置为 4，让模型更灵活地调整注意力或激活值。\n",
    "\n",
    "对任务要求较稳健时：增大 ia3_alpha，例如设置为 16，避免过大改动。\n",
    "\n",
    "默认设置：bias=\"none\"\n",
    "\n",
    "none or all or lora_only\n",
    "\n",
    "all-对所有 Transformer 模块的偏置项进行调整。\n",
    "\n",
    "lora_only-偏置项仅在与 IA3 相关的模块（如 Value 投影矩阵）中进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"/home/zhuyao/Sunpeng/models/qwen_2B_instruct\", torch_dtype=\"auto\", device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLModel(\n",
       "  (embed_tokens): Embedding(151936, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "      (self_attn): Qwen2VLSdpaAttention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
